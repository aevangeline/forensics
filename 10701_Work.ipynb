{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "10701 - Work",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aevangeline/forensics/blob/master/10701_Work.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbNn34dodj8B",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Aurelia and Caroline - 10701"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ei2_bEpaR2FO",
        "colab_type": "code",
        "outputId": "79d1f903-c65b-4dd6-922b-d56b9bae4901",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2018 NVIDIA Corporation\n",
            "Built on Sat_Aug_25_21:08:01_CDT_2018\n",
            "Cuda compilation tools, release 10.0, V10.0.130\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s08zJMHfRgvB",
        "colab_type": "code",
        "outputId": "1216a6a4-23f2-4959-c7fe-55937f40451b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "!pip install --extra-index-url https://developer.download.nvidia.com/compute/redist/cuda/10.0 nvidia-dali"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://developer.download.nvidia.com/compute/redist/cuda/10.0\n",
            "Requirement already satisfied: nvidia-dali in /usr/local/lib/python3.6/dist-packages (0.16.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from nvidia-dali) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhJoenqRRrGv",
        "colab_type": "code",
        "outputId": "1518f95b-cde7-471e-8884-cec42e25872d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "from zipfile import ZipFile\n",
        "import pathlib\n",
        "from urllib.request import urlretrieve\n",
        "from collections import defaultdict\n",
        "import os\n",
        "from os import remove\n",
        "import os.path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import shutil as sh\n",
        "import torch\n",
        "import glob\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import nvidia.dali.ops as ops\n",
        "import nvidia.dali.types as types\n",
        "from nvidia.dali.pipeline import Pipeline\n",
        "from nvidia.dali.plugin.pytorch import DALIClassificationIterator as PyTorchIterator\n",
        "from random import shuffle, random\n",
        "import math\n",
        "import time\n",
        "import csv\n",
        "import copy\n",
        "\n",
        "from IPython.display import display, HTML\n",
        "js = ('<script>function ConnectButton(){ '\n",
        "        'console.log(\"Connect pushed\"); '\n",
        "        'document.querySelector(\"#connect\").click()} '\n",
        "        'setInterval(ConnectButton,3000);</script>')\n",
        "display(HTML(js))\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<script>function ConnectButton(){ console.log(\"Connect pushed\"); document.querySelector(\"#connect\").click()} setInterval(ConnectButton,3000);</script>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsHAEto1543H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FID_DIRECTORY = pathlib.Path(\"FID-300\")\n",
        "FID_LABELS = FID_DIRECTORY / \"label_table.csv\"\n",
        "FID_SOURCE_URL = \"https://fid.dmi.unibas.ch/FID-300.zip\"\n",
        "TRAIN_DIR = \"FID-300/references/\"\n",
        "TEST_DIR = \"FID-300/tracks_cropped/\"\n",
        "NUM_EPOCHS = 100\n",
        "NUM_CLASSES = 1175\n",
        "INPUT_SIZE = 224\n",
        "BATCH_SIZE = 64\n",
        "MODEL_NAME = \"resnet\"\n",
        "FEATURE_EXTRACT=False\n",
        "USE_PRETRAINED=False\n",
        "db_folder = pathlib.Path(TRAIN_DIR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsEh9EEYC4Ur",
        "colab_type": "code",
        "outputId": "b1e0621b-4763-4c88-ca25-259a9c714747",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def fetch_FID_300_data():\n",
        "    \"\"\"Downloads and extracts FID-300 data to a local folder\"\"\"\n",
        "    if FID_DIRECTORY.exists():\n",
        "        print(\"FID-300 Database already exists\")\n",
        "        return\n",
        "    print(\"Downloading FID_300\")\n",
        "    local_file, _ = urlretrieve(FID_SOURCE_URL)\n",
        "    with ZipFile(local_file) as archive:\n",
        "        print(\"Extracting FID-300\")\n",
        "        archive.extractall()\n",
        "    remove(local_file)\n",
        "\n",
        "fetch_FID_300_data()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FID-300 Database already exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojc6z_bs-Q1J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ExternalInputIterator(object):\n",
        "    def __init__(self, batch_size, image_dir, repeat = 32):\n",
        "        self.images_dir = pathlib.Path(image_dir)\n",
        "        self.images = list(glob.iglob(str(self.images_dir/\"*\"))) \n",
        "        self.batch_size = batch_size \n",
        "        self.repeat = repeat\n",
        "        self.queue = self.images * self.repeat\n",
        "        shuffle(self.queue)\n",
        "        self.i = 0  \n",
        "\n",
        "    def __iter__(self):\n",
        "        self.i = 0\n",
        "        shuffle(self.queue)\n",
        "        return self\n",
        "\n",
        "    @property\n",
        "    def size(self,):\n",
        "      return len(self.images) * self.repeat\n",
        "\n",
        "    def __next__(self):\n",
        "        batch = []\n",
        "        labels = []\n",
        "        if self.i >= len(self.queue):\n",
        "          raise StopIteration\n",
        "        for _ in range(self.batch_size):\n",
        "          while self.i >= len(self.queue):\n",
        "            self.i -= 1\n",
        "          img = self.queue[self.i]\n",
        "          fname = pathlib.Path(img)\n",
        "          label = np.array(int(fname.stem) - 1, dtype = np.uint8)\n",
        "          with open(fname, 'rb') as f:\n",
        "            buff = np.frombuffer(f.read(), dtype = np.uint8)\n",
        "            batch.append(buff)\n",
        "          labels.append(label)\n",
        "          self.i += 1    \n",
        "        return (batch, labels)\n",
        "\n",
        "    next = __next__"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3eIOKVQSMwx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AugmentationPipeline(Pipeline):\n",
        "    def __init__(self, batch_size, num_threads, device_id = 0,\n",
        "                 examples_per_image = 1000, folder = db_folder,\n",
        "                 pipelined = True, exec_async = True):\n",
        "        super(AugmentationPipeline, self).__init__(batch_size, num_threads,\n",
        "                                                   device_id, seed = 12,\n",
        "                                                   exec_pipelined=pipelined,\n",
        "                                                   exec_async=exec_async)\n",
        "        self.external_data = ExternalInputIterator(batch_size,\n",
        "                                                   folder,\n",
        "                                                   examples_per_image)\n",
        "        self.input = ops.ExternalSource()\n",
        "        self.input_label = ops.ExternalSource()\n",
        "        self.iterator = iter(self.external_data)\n",
        "        self.decode = ops.ImageDecoderRandomCrop(device = \"mixed\", output_type = types.RGB,\n",
        "            random_aspect_ratio=[0.8, 1.25],\n",
        "            random_area=[0.6, 1.0],\n",
        "            num_attempts=100)\n",
        "        self.augmentations = {}\n",
        "        # input is sampled randomly for output pixel's neighbourhood\n",
        "        self.augmentations[\"jitter\"] = (0.3, ops.Jitter(device = \"gpu\"))\n",
        "        # transforms sampling coordinates to produce wavy patterns\n",
        "        self.augmentations[\"water\"] = (0.2, ops.Water(device = \"gpu\"))\n",
        "        # applies fisheye distortion\n",
        "        self.augmentations[\"sphere\"] = (0.3, ops.Sphere(device = \"gpu\"))\n",
        "        # rotates the image, enlarging the canvas\n",
        "        self.rotation_rng = ops.Uniform(range=(-180.00, 180.00))\n",
        "        self.rotate = ops.Rotate(device = \"gpu\",\n",
        "                                 interp_type = types.INTERP_LINEAR,\n",
        "                                 fill_value = 0)\n",
        "        # param resizes the image so that the shorter edge is exactly 400px long\n",
        "        self.resize = ops.Resize(device = \"gpu\", resize_x = INPUT_SIZE, resize_y = INPUT_SIZE)\n",
        "        # param flips the image\n",
        "        self.flip_rng = ops.CoinFlip()\n",
        "        self.hflip_rng = ops.CoinFlip()\n",
        "        self.flip = ops.Flip(device = \"gpu\")\n",
        "        self.bri_con_rng = ops.Uniform(range = (.5, 2))\n",
        "        self.saturation_rng = ops.Uniform(range = (.2, 1))\n",
        "        self.color_twist = ops.ColorTwist(device = \"gpu\")\n",
        "        self.iter = 0\n",
        "\n",
        "    def define_graph(self):\n",
        "        self.jpegs = self.input(name=\"Reader\")\n",
        "        self.labels = self.input_label(name=\"Reader\")\n",
        "        output_labels = []\n",
        "        images = self.decode(self.jpegs).gpu()\n",
        "        transformed = images\n",
        "        for thresh, op in self.augmentations.values():\n",
        "          if random() < thresh:\n",
        "            transformed = op(images)\n",
        "        #transformed = self.flip(transformed,\n",
        "        #                        vertical = self.flip_rng(),\n",
        "        #                        horizontal = self.hflip_rng())\n",
        "        transformed = self.rotate(transformed, angle = self.rotation_rng())\n",
        "        #transformed = self.color_twist(transformed, brightness=self.bri_con_rng(),\n",
        "        #                                contrast=self.bri_con_rng(),\n",
        "        #                                saturation=self.saturation_rng())\n",
        "        transformed = self.resize(transformed)\n",
        "        return (transformed, self.labels)\n",
        "\n",
        "    @property\n",
        "    def iter_size(self,):\n",
        "      return self.external_data.size\n",
        "\n",
        "\n",
        "    def iter_setup(self):\n",
        "      try:\n",
        "        (images, labels) = self.iterator.next()\n",
        "        self.feed_input(self.jpegs, images)\n",
        "        self.feed_input(self.labels, labels)\n",
        "      except StopIteration:\n",
        "        self.iterator = iter(self.external_data)\n",
        "        raise StopIteration\n",
        "      \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IV9Z7HCyodDN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_images():\n",
        "    transformations = transforms.Compose([\n",
        "        transforms.Resize((INPUT_SIZE, INPUT_SIZE)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[\n",
        "                             0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    train_pipe = AugmentationPipeline(BATCH_SIZE, 8, examples_per_image=50)\n",
        "    train_pii = PyTorchIterator(train_pipe,\n",
        "                                          size=train_pipe.iter_size,\n",
        "                                          last_batch_padded=True,\n",
        "                                          fill_last_batch=True)\n",
        "    validate_pipe = AugmentationPipeline(BATCH_SIZE, 8, examples_per_image=5)\n",
        "    validate_pii = PyTorchIterator(validate_pipe,\n",
        "                                             size=validate_pipe.iter_size,\n",
        "                                             last_batch_padded=True,\n",
        "                                             fill_last_batch=True)\n",
        "\n",
        "    test_data = datasets.ImageFolder(TEST_DIR, transform=transformations)\n",
        "    num_train = len(test_data)\n",
        "    indices = list(range(num_train))\n",
        "\n",
        "    test_sampler = SubsetRandomSampler(indices)\n",
        "    testloader = torch.utils.data.DataLoader(test_data,\n",
        "                                             sampler=test_sampler, batch_size=64)\n",
        "    dataloaders_dict = {'train':train_pii, 'val' : validate_pii,  'test':testloader}\n",
        "    return dataloaders_dict, {\"train\" : train_pipe.iter_size, \"val\" : validate_pipe.iter_size, \"test\" : num_train}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPlLbD0F3Z5X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_labels():\n",
        "    labels = pd.read_csv(FID_LABELS, delimiter=\",\", header=None,\n",
        "                         dtype=np.dtype(int), names=['id', 'label'])\n",
        "    return labels\n",
        "\n",
        "labels = load_labels()\n",
        "\n",
        "\n",
        "def organize_files(label_df):\n",
        "  \"\"\" Moves all pngs in tracked_cropped into subfolders by label (for PyTorch Image Folder) \"\"\"\n",
        "  test_dir = pathlib.Path(TEST_DIR)\n",
        "  files = glob.glob(str(test_dir / \"*.jpg\"))\n",
        "  for i in range(len(files)):\n",
        "    f = pathlib.Path(files[i])\n",
        "    fname = f.name\n",
        "    id = int(f.stem)\n",
        "    label = label_df[\"label\"].iloc[id-1]\n",
        "    new_dir = test_dir / str(label - 1)\n",
        "    new_dir.mkdir(exist_ok=True)\n",
        "    new_file = new_dir / fname\n",
        "    sh.move(f, new_file)\n",
        "\n",
        "organize_files(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8r2lin8ffu0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(model, dataloaders_info, criterion, optimizer, num_epochs=25, is_inception=False):\n",
        "    since = time.time()\n",
        "    val_acc_history = []\n",
        "    train_acc_history = []\n",
        "    test_acc_history = []\n",
        "    dataloaders, datasizes = dataloaders_info\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val', 'test']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for data in dataloaders[phase]:\n",
        "                inputs, labels = (None, None)\n",
        "                if phase != \"test\":\n",
        "                  inputs = data[0][\"data\"]\n",
        "                  labels = data[0][\"label\"]\n",
        "                else:\n",
        "                  inputs, labels = data \n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                if phase != 'test':\n",
        "                    inputs = torch.reshape(inputs,(BATCH_SIZE, 3, INPUT_SIZE, INPUT_SIZE))\n",
        "                    inputs = inputs.float()\n",
        "                    labels = labels.to(torch.int64)\n",
        "                    labels = torch.reshape(labels, (BATCH_SIZE,))\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    # Get model outputs and calculate loss\n",
        "                    # Special case for inception because in training it has an auxiliary output. In train\n",
        "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
        "                    #   but in testing we only consider the final output.\n",
        "                    if is_inception and phase == 'train':\n",
        "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
        "                        outputs, aux_outputs = model(inputs)\n",
        "                        loss1 = criterion(outputs, labels)\n",
        "                        loss2 = criterion(aux_outputs, labels)\n",
        "                        loss = loss1 + 0.4*loss2\n",
        "                    else:\n",
        "                        outputs = model(inputs)\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                torch.cuda.synchronize(device=None)\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            data_size = datasizes[phase]\n",
        "            if phase != \"test\":\n",
        "              dataloaders[phase].reset()\n",
        "\n",
        "\n",
        "\n",
        "            epoch_loss = running_loss / data_size\n",
        "            epoch_acc = float(running_corrects) / data_size\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            if phase == 'val':\n",
        "                val_acc_history.append(epoch_acc)\n",
        "            if phase == 'train':\n",
        "                train_acc_history.append(epoch_acc)\n",
        "            if phase == \"test\":\n",
        "                test_acc_history.append(epoch_acc)\n",
        "                \n",
        "        print()\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, val_acc_history, train_acc_history, test_acc_history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVv4Imzbn-b2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_model(MODEL_NAME, NUM_CLASSES, FEATURE_EXTRACT=False, USE_PRETRAINED=False):\n",
        "    model_ft = None\n",
        "    input_size = 0\n",
        "\n",
        "    if MODEL_NAME == \"resnet\":\n",
        "        model_ft = models.resnet18(pretrained=USE_PRETRAINED)\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs, NUM_CLASSES)\n",
        "        input_size = 224\n",
        "\n",
        "    elif MODEL_NAME == \"alexnet\":\n",
        "        model_ft = models.alexnet(pretrained=USE_PRETRAINED)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,NUM_CLASSES)\n",
        "        input_size = 224\n",
        "\n",
        "    elif MODEL_NAME == \"vgg\":\n",
        "        model_ft = models.vgg11_bn(pretrained=USE_PRETRAINED)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,NUM_CLASSES)\n",
        "        input_size = 224\n",
        "\n",
        "    elif MODEL_NAME == \"squeezenet\":\n",
        "        model_ft = models.squeezenet1_0(pretrained=USE_PRETRAINED)\n",
        "        model_ft.classifier[1] = nn.Conv2d(512, NUM_CLASSES, kernel_size=(1,1), stride=(1,1))\n",
        "        model_ft.num_classes = NUM_CLASSES\n",
        "        input_size = 224\n",
        "\n",
        "    elif MODEL_NAME == \"densenet\":\n",
        "        \"\"\" Densenet\n",
        "        \"\"\"\n",
        "        model_ft = models.densenet121(pretrained=USE_PRETRAINED)\n",
        "        num_ftrs = model_ft.classifier.in_features\n",
        "        model_ft.classifier = nn.Linear(num_ftrs, NUM_CLASSES)\n",
        "        input_size = 224\n",
        "\n",
        "    elif MODEL_NAME == \"inception\":\n",
        "        \"\"\" Inception v3\n",
        "        Be careful, expects (299,299) sized images and has auxiliary output\n",
        "        \"\"\"\n",
        "        model_ft = models.inception_v3(pretrained=USE_PRETRAINED)\n",
        "        # Handle the auxilary net\n",
        "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
        "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, NUM_CLASSES)\n",
        "        # Handle the primary net\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs,NUM_CLASSES)\n",
        "        input_size = 299\n",
        "\n",
        "    else:\n",
        "        print(\"Invalid model name, exiting...\")\n",
        "        exit()\n",
        "\n",
        "    return model_ft, input_size\n",
        "\n",
        "# Initialize the model for this run\n",
        "model_ft, input_size = initialize_model(MODEL_NAME, NUM_CLASSES, \n",
        "                                        FEATURE_EXTRACT, USE_PRETRAINED)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5WMFl8-vTAc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Send the model to GPU\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "# Gather the parameters to be optimized/updated in this run. If we are\n",
        "#  finetuning we will be updating all parameters. However, if we are\n",
        "#  doing feature extract method, we will only update the parameters\n",
        "#  that we have just initialized, i.e. the parameters with requires_grad\n",
        "#  is True.\n",
        "params_to_update = model_ft.parameters()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRTxs-SivimF",
        "colab_type": "code",
        "outputId": "89c9aa10-810f-4d69-f87c-c06ba31e1014",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "fetch_FID_300_data()\n",
        "labels = load_labels()\n",
        "organize_files(labels)\n",
        "dataloaders_info = process_images()\n",
        "\n",
        "# Setup the loss fxn\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "model_ft, hist, train_hist, test_hist = train_model(model_ft, dataloaders_info, criterion, optimizer_ft, \n",
        "                             num_epochs=NUM_EPOCHS, is_inception=(MODEL_NAME==\"inception\"))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FID-300 Database already exists\n",
            "Epoch 1/100\n",
            "----------\n",
            "train Loss: 6.5482 Acc: 0.0028\n",
            "val Loss: 6.2178 Acc: 0.0034\n",
            "test Loss: 6.0209 Acc: 0.0033\n",
            "\n",
            "Epoch 2/100\n",
            "----------\n",
            "train Loss: 5.8871 Acc: 0.0043\n",
            "val Loss: 5.9155 Acc: 0.0085\n",
            "test Loss: 5.7740 Acc: 0.0033\n",
            "\n",
            "Epoch 3/100\n",
            "----------\n",
            "train Loss: 5.7024 Acc: 0.0057\n",
            "val Loss: 5.8059 Acc: 0.0102\n",
            "test Loss: 6.2732 Acc: 0.0200\n",
            "\n",
            "Epoch 4/100\n",
            "----------\n",
            "train Loss: 5.6131 Acc: 0.0065\n",
            "val Loss: 5.7326 Acc: 0.0085\n",
            "test Loss: 6.7462 Acc: 0.0067\n",
            "\n",
            "Epoch 5/100\n",
            "----------\n",
            "train Loss: 5.5533 Acc: 0.0077\n",
            "val Loss: 5.6763 Acc: 0.0111\n",
            "test Loss: 7.3060 Acc: 0.0067\n",
            "\n",
            "Epoch 6/100\n",
            "----------\n",
            "train Loss: 5.4962 Acc: 0.0095\n",
            "val Loss: 5.6310 Acc: 0.0094\n",
            "test Loss: 7.2886 Acc: 0.0000\n",
            "\n",
            "Epoch 7/100\n",
            "----------\n",
            "train Loss: 5.4537 Acc: 0.0100\n",
            "val Loss: 5.5849 Acc: 0.0111\n",
            "test Loss: 7.7427 Acc: 0.0067\n",
            "\n",
            "Epoch 8/100\n",
            "----------\n",
            "train Loss: 5.4118 Acc: 0.0115\n",
            "val Loss: 5.5480 Acc: 0.0136\n",
            "test Loss: 9.3071 Acc: 0.0000\n",
            "\n",
            "Epoch 9/100\n",
            "----------\n",
            "train Loss: 5.3651 Acc: 0.0135\n",
            "val Loss: 5.4867 Acc: 0.0111\n",
            "test Loss: 9.8412 Acc: 0.0000\n",
            "\n",
            "Epoch 10/100\n",
            "----------\n",
            "train Loss: 5.3243 Acc: 0.0157\n",
            "val Loss: 5.4334 Acc: 0.0162\n",
            "test Loss: 12.2912 Acc: 0.0000\n",
            "\n",
            "Epoch 11/100\n",
            "----------\n",
            "train Loss: 5.2842 Acc: 0.0166\n",
            "val Loss: 5.3840 Acc: 0.0179\n",
            "test Loss: 10.5493 Acc: 0.0067\n",
            "\n",
            "Epoch 12/100\n",
            "----------\n",
            "train Loss: 5.2458 Acc: 0.0180\n",
            "val Loss: 5.6652 Acc: 0.0145\n",
            "test Loss: 12.6359 Acc: 0.0067\n",
            "\n",
            "Epoch 13/100\n",
            "----------\n",
            "train Loss: 5.2078 Acc: 0.0174\n",
            "val Loss: 5.3945 Acc: 0.0179\n",
            "test Loss: 12.5266 Acc: 0.0067\n",
            "\n",
            "Epoch 14/100\n",
            "----------\n",
            "train Loss: 5.1655 Acc: 0.0201\n",
            "val Loss: 5.3652 Acc: 0.0255\n",
            "test Loss: 12.1289 Acc: 0.0200\n",
            "\n",
            "Epoch 15/100\n",
            "----------\n",
            "train Loss: 5.1288 Acc: 0.0205\n",
            "val Loss: 5.2108 Acc: 0.0281\n",
            "test Loss: 13.3217 Acc: 0.0000\n",
            "\n",
            "Epoch 16/100\n",
            "----------\n",
            "train Loss: 5.0861 Acc: 0.0223\n",
            "val Loss: 5.4543 Acc: 0.0196\n",
            "test Loss: 13.1825 Acc: 0.0000\n",
            "\n",
            "Epoch 17/100\n",
            "----------\n",
            "train Loss: 5.0630 Acc: 0.0225\n",
            "val Loss: 5.1392 Acc: 0.0332\n",
            "test Loss: 13.7903 Acc: 0.0000\n",
            "\n",
            "Epoch 18/100\n",
            "----------\n",
            "train Loss: 5.0275 Acc: 0.0259\n",
            "val Loss: 5.1868 Acc: 0.0272\n",
            "test Loss: 12.9067 Acc: 0.0000\n",
            "\n",
            "Epoch 19/100\n",
            "----------\n",
            "train Loss: 4.9935 Acc: 0.0297\n",
            "val Loss: 5.7691 Acc: 0.0298\n",
            "test Loss: 12.4328 Acc: 0.0000\n",
            "\n",
            "Epoch 20/100\n",
            "----------\n",
            "train Loss: 4.9366 Acc: 0.0289\n",
            "val Loss: 5.1134 Acc: 0.0204\n",
            "test Loss: 11.4355 Acc: 0.0000\n",
            "\n",
            "Epoch 21/100\n",
            "----------\n",
            "train Loss: 4.9130 Acc: 0.0277\n",
            "val Loss: 5.0196 Acc: 0.0409\n",
            "test Loss: 12.7350 Acc: 0.0000\n",
            "\n",
            "Epoch 22/100\n",
            "----------\n",
            "train Loss: 4.8639 Acc: 0.0340\n",
            "val Loss: 5.0428 Acc: 0.0315\n",
            "test Loss: 14.3935 Acc: 0.0000\n",
            "\n",
            "Epoch 23/100\n",
            "----------\n",
            "train Loss: 4.8264 Acc: 0.0352\n",
            "val Loss: 4.9948 Acc: 0.0281\n",
            "test Loss: 14.2034 Acc: 0.0000\n",
            "\n",
            "Epoch 24/100\n",
            "----------\n",
            "train Loss: 4.7936 Acc: 0.0340\n",
            "val Loss: 5.0740 Acc: 0.0306\n",
            "test Loss: 18.0238 Acc: 0.0000\n",
            "\n",
            "Epoch 25/100\n",
            "----------\n",
            "train Loss: 4.7447 Acc: 0.0352\n",
            "val Loss: 5.0444 Acc: 0.0349\n",
            "test Loss: 12.5171 Acc: 0.0000\n",
            "\n",
            "Epoch 26/100\n",
            "----------\n",
            "train Loss: 4.7082 Acc: 0.0395\n",
            "val Loss: 4.7380 Acc: 0.0485\n",
            "test Loss: 17.7876 Acc: 0.0000\n",
            "\n",
            "Epoch 27/100\n",
            "----------\n",
            "train Loss: 4.6641 Acc: 0.0434\n",
            "val Loss: 5.1239 Acc: 0.0306\n",
            "test Loss: 16.8178 Acc: 0.0000\n",
            "\n",
            "Epoch 28/100\n",
            "----------\n",
            "train Loss: 4.6315 Acc: 0.0455\n",
            "val Loss: 4.7796 Acc: 0.0528\n",
            "test Loss: 19.8566 Acc: 0.0000\n",
            "\n",
            "Epoch 29/100\n",
            "----------\n",
            "train Loss: 4.6017 Acc: 0.0470\n",
            "val Loss: 4.8994 Acc: 0.0460\n",
            "test Loss: 14.7549 Acc: 0.0000\n",
            "\n",
            "Epoch 30/100\n",
            "----------\n",
            "train Loss: 4.5523 Acc: 0.0524\n",
            "val Loss: 5.0232 Acc: 0.0468\n",
            "test Loss: 15.5077 Acc: 0.0000\n",
            "\n",
            "Epoch 31/100\n",
            "----------\n",
            "train Loss: 4.5224 Acc: 0.0506\n",
            "val Loss: 4.8506 Acc: 0.0383\n",
            "test Loss: 16.6283 Acc: 0.0000\n",
            "\n",
            "Epoch 32/100\n",
            "----------\n",
            "train Loss: 4.4813 Acc: 0.0568\n",
            "val Loss: 4.9397 Acc: 0.0485\n",
            "test Loss: 14.1093 Acc: 0.0000\n",
            "\n",
            "Epoch 33/100\n",
            "----------\n",
            "train Loss: 4.4557 Acc: 0.0590\n",
            "val Loss: 5.0311 Acc: 0.0417\n",
            "test Loss: 17.2653 Acc: 0.0000\n",
            "\n",
            "Epoch 34/100\n",
            "----------\n",
            "train Loss: 4.4200 Acc: 0.0605\n",
            "val Loss: 4.9008 Acc: 0.0468\n",
            "test Loss: 16.6617 Acc: 0.0000\n",
            "\n",
            "Epoch 35/100\n",
            "----------\n",
            "train Loss: 4.3960 Acc: 0.0586\n",
            "val Loss: 4.5747 Acc: 0.0604\n",
            "test Loss: 20.6818 Acc: 0.0000\n",
            "\n",
            "Epoch 36/100\n",
            "----------\n",
            "train Loss: 4.3564 Acc: 0.0617\n",
            "val Loss: 4.5502 Acc: 0.0638\n",
            "test Loss: 17.2701 Acc: 0.0000\n",
            "\n",
            "Epoch 37/100\n",
            "----------\n",
            "train Loss: 4.3269 Acc: 0.0668\n",
            "val Loss: 4.5486 Acc: 0.0596\n",
            "test Loss: 15.4288 Acc: 0.0000\n",
            "\n",
            "Epoch 38/100\n",
            "----------\n",
            "train Loss: 4.3042 Acc: 0.0665\n",
            "val Loss: 4.3730 Acc: 0.0791\n",
            "test Loss: 17.1832 Acc: 0.0000\n",
            "\n",
            "Epoch 39/100\n",
            "----------\n",
            "train Loss: 4.2696 Acc: 0.0695\n",
            "val Loss: 4.4738 Acc: 0.0689\n",
            "test Loss: 19.1530 Acc: 0.0000\n",
            "\n",
            "Epoch 40/100\n",
            "----------\n",
            "train Loss: 4.2631 Acc: 0.0691\n",
            "val Loss: 4.8221 Acc: 0.0494\n",
            "test Loss: 16.2524 Acc: 0.0000\n",
            "\n",
            "Epoch 41/100\n",
            "----------\n",
            "train Loss: 4.2133 Acc: 0.0745\n",
            "val Loss: 5.1477 Acc: 0.0315\n",
            "test Loss: 15.0178 Acc: 0.0000\n",
            "\n",
            "Epoch 42/100\n",
            "----------\n",
            "train Loss: 4.1670 Acc: 0.0764\n",
            "val Loss: 4.6248 Acc: 0.0647\n",
            "test Loss: 16.6134 Acc: 0.0000\n",
            "\n",
            "Epoch 43/100\n",
            "----------\n",
            "train Loss: 4.1478 Acc: 0.0767\n",
            "val Loss: 4.7652 Acc: 0.0613\n",
            "test Loss: 16.2665 Acc: 0.0000\n",
            "\n",
            "Epoch 44/100\n",
            "----------\n",
            "train Loss: 4.1406 Acc: 0.0794\n",
            "val Loss: 4.3024 Acc: 0.0732\n",
            "test Loss: 15.8365 Acc: 0.0000\n",
            "\n",
            "Epoch 45/100\n",
            "----------\n",
            "train Loss: 4.1098 Acc: 0.0827\n",
            "val Loss: 4.9368 Acc: 0.0391\n",
            "test Loss: 14.0144 Acc: 0.0000\n",
            "\n",
            "Epoch 46/100\n",
            "----------\n",
            "train Loss: 4.0929 Acc: 0.0895\n",
            "val Loss: 4.1904 Acc: 0.1047\n",
            "test Loss: 15.0720 Acc: 0.0000\n",
            "\n",
            "Epoch 47/100\n",
            "----------\n",
            "train Loss: 4.0573 Acc: 0.0866\n",
            "val Loss: 4.7062 Acc: 0.0621\n",
            "test Loss: 15.0699 Acc: 0.0000\n",
            "\n",
            "Epoch 48/100\n",
            "----------\n",
            "train Loss: 4.0115 Acc: 0.0949\n",
            "val Loss: 4.5800 Acc: 0.0715\n",
            "test Loss: 18.1839 Acc: 0.0000\n",
            "\n",
            "Epoch 49/100\n",
            "----------\n",
            "train Loss: 4.0062 Acc: 0.0948\n",
            "val Loss: 4.0788 Acc: 0.1013\n",
            "test Loss: 15.7689 Acc: 0.0000\n",
            "\n",
            "Epoch 50/100\n",
            "----------\n",
            "train Loss: 3.9783 Acc: 0.0946\n",
            "val Loss: 3.8501 Acc: 0.1345\n",
            "test Loss: 14.2174 Acc: 0.0000\n",
            "\n",
            "Epoch 51/100\n",
            "----------\n",
            "train Loss: 3.9877 Acc: 0.0946\n",
            "val Loss: 4.2005 Acc: 0.0945\n",
            "test Loss: 17.2170 Acc: 0.0000\n",
            "\n",
            "Epoch 52/100\n",
            "----------\n",
            "train Loss: 3.9358 Acc: 0.0985\n",
            "val Loss: 4.6309 Acc: 0.0596\n",
            "test Loss: 15.0469 Acc: 0.0000\n",
            "\n",
            "Epoch 53/100\n",
            "----------\n",
            "train Loss: 3.9323 Acc: 0.1016\n",
            "val Loss: 4.2446 Acc: 0.0936\n",
            "test Loss: 11.9212 Acc: 0.0000\n",
            "\n",
            "Epoch 54/100\n",
            "----------\n",
            "train Loss: 3.8769 Acc: 0.1075\n",
            "val Loss: 5.4504 Acc: 0.0400\n",
            "test Loss: 13.7352 Acc: 0.0000\n",
            "\n",
            "Epoch 55/100\n",
            "----------\n",
            "train Loss: 3.8658 Acc: 0.1094\n",
            "val Loss: 4.2916 Acc: 0.0860\n",
            "test Loss: 14.5107 Acc: 0.0000\n",
            "\n",
            "Epoch 56/100\n",
            "----------\n",
            "train Loss: 3.8394 Acc: 0.1100\n",
            "val Loss: 4.9870 Acc: 0.0545\n",
            "test Loss: 15.3917 Acc: 0.0000\n",
            "\n",
            "Epoch 57/100\n",
            "----------\n",
            "train Loss: 3.8033 Acc: 0.1181\n",
            "val Loss: 4.1160 Acc: 0.0945\n",
            "test Loss: 15.3501 Acc: 0.0000\n",
            "\n",
            "Epoch 58/100\n",
            "----------\n",
            "train Loss: 3.7914 Acc: 0.1186\n",
            "val Loss: 3.9554 Acc: 0.1251\n",
            "test Loss: 13.8615 Acc: 0.0000\n",
            "\n",
            "Epoch 59/100\n",
            "----------\n",
            "train Loss: 3.7642 Acc: 0.1236\n",
            "val Loss: 3.7145 Acc: 0.1591\n",
            "test Loss: 14.3839 Acc: 0.0000\n",
            "\n",
            "Epoch 60/100\n",
            "----------\n",
            "train Loss: 3.7430 Acc: 0.1288\n",
            "val Loss: 4.0580 Acc: 0.1064\n",
            "test Loss: 16.3872 Acc: 0.0000\n",
            "\n",
            "Epoch 61/100\n",
            "----------\n",
            "train Loss: 3.7224 Acc: 0.1252\n",
            "val Loss: 4.8907 Acc: 0.0630\n",
            "test Loss: 15.2711 Acc: 0.0067\n",
            "\n",
            "Epoch 62/100\n",
            "----------\n",
            "train Loss: 3.6952 Acc: 0.1289\n",
            "val Loss: 3.6576 Acc: 0.1574\n",
            "test Loss: 14.0811 Acc: 0.0000\n",
            "\n",
            "Epoch 63/100\n",
            "----------\n",
            "train Loss: 3.6930 Acc: 0.1354\n",
            "val Loss: 6.0208 Acc: 0.0340\n",
            "test Loss: 14.5573 Acc: 0.0000\n",
            "\n",
            "Epoch 64/100\n",
            "----------\n",
            "train Loss: 3.6494 Acc: 0.1349\n",
            "val Loss: 4.1674 Acc: 0.0970\n",
            "test Loss: 17.7089 Acc: 0.0000\n",
            "\n",
            "Epoch 65/100\n",
            "----------\n",
            "train Loss: 3.5978 Acc: 0.1427\n",
            "val Loss: 4.1237 Acc: 0.1123\n",
            "test Loss: 14.5893 Acc: 0.0000\n",
            "\n",
            "Epoch 66/100\n",
            "----------\n",
            "train Loss: 3.6030 Acc: 0.1418\n",
            "val Loss: 4.6633 Acc: 0.0613\n",
            "test Loss: 18.5890 Acc: 0.0000\n",
            "\n",
            "Epoch 67/100\n",
            "----------\n",
            "train Loss: 3.5525 Acc: 0.1550\n",
            "val Loss: 5.2253 Acc: 0.0443\n",
            "test Loss: 15.6431 Acc: 0.0067\n",
            "\n",
            "Epoch 68/100\n",
            "----------\n",
            "train Loss: 3.5575 Acc: 0.1466\n",
            "val Loss: 4.4420 Acc: 0.0919\n",
            "test Loss: 16.6034 Acc: 0.0000\n",
            "\n",
            "Epoch 69/100\n",
            "----------\n",
            "train Loss: 3.4942 Acc: 0.1626\n",
            "val Loss: 4.1503 Acc: 0.1021\n",
            "test Loss: 16.6017 Acc: 0.0000\n",
            "\n",
            "Epoch 70/100\n",
            "----------\n",
            "train Loss: 3.5090 Acc: 0.1585\n",
            "val Loss: 3.9765 Acc: 0.1191\n",
            "test Loss: 19.4446 Acc: 0.0000\n",
            "\n",
            "Epoch 71/100\n",
            "----------\n",
            "train Loss: 3.5031 Acc: 0.1565\n",
            "val Loss: 4.7417 Acc: 0.0579\n",
            "test Loss: 18.7184 Acc: 0.0000\n",
            "\n",
            "Epoch 72/100\n",
            "----------\n",
            "train Loss: 3.4270 Acc: 0.1700\n",
            "val Loss: 4.4102 Acc: 0.0894\n",
            "test Loss: 17.9317 Acc: 0.0000\n",
            "\n",
            "Epoch 73/100\n",
            "----------\n",
            "train Loss: 3.4197 Acc: 0.1652\n",
            "val Loss: 4.8501 Acc: 0.0749\n",
            "test Loss: 17.3208 Acc: 0.0000\n",
            "\n",
            "Epoch 74/100\n",
            "----------\n",
            "train Loss: 3.4107 Acc: 0.1719\n",
            "val Loss: 6.5995 Acc: 0.0315\n",
            "test Loss: 19.7451 Acc: 0.0000\n",
            "\n",
            "Epoch 75/100\n",
            "----------\n",
            "train Loss: 3.3745 Acc: 0.1758\n",
            "val Loss: 3.9565 Acc: 0.1268\n",
            "test Loss: 17.8855 Acc: 0.0000\n",
            "\n",
            "Epoch 76/100\n",
            "----------\n",
            "train Loss: 3.3625 Acc: 0.1750\n",
            "val Loss: 3.4026 Acc: 0.2009\n",
            "test Loss: 20.0088 Acc: 0.0000\n",
            "\n",
            "Epoch 77/100\n",
            "----------\n",
            "train Loss: 3.3327 Acc: 0.1883\n",
            "val Loss: 3.5226 Acc: 0.1906\n",
            "test Loss: 17.7338 Acc: 0.0000\n",
            "\n",
            "Epoch 78/100\n",
            "----------\n",
            "train Loss: 3.3006 Acc: 0.1909\n",
            "val Loss: 4.7109 Acc: 0.0851\n",
            "test Loss: 19.1883 Acc: 0.0000\n",
            "\n",
            "Epoch 79/100\n",
            "----------\n",
            "train Loss: 3.2561 Acc: 0.1933\n",
            "val Loss: 4.9899 Acc: 0.0570\n",
            "test Loss: 17.9105 Acc: 0.0067\n",
            "\n",
            "Epoch 80/100\n",
            "----------\n",
            "train Loss: 3.2290 Acc: 0.2029\n",
            "val Loss: 3.6563 Acc: 0.1872\n",
            "test Loss: 18.2869 Acc: 0.0067\n",
            "\n",
            "Epoch 81/100\n",
            "----------\n",
            "train Loss: 3.2069 Acc: 0.2063\n",
            "val Loss: 3.0553 Acc: 0.2706\n",
            "test Loss: 18.1892 Acc: 0.0000\n",
            "\n",
            "Epoch 82/100\n",
            "----------\n",
            "train Loss: 3.1856 Acc: 0.2120\n",
            "val Loss: 3.3372 Acc: 0.2068\n",
            "test Loss: 19.7379 Acc: 0.0000\n",
            "\n",
            "Epoch 83/100\n",
            "----------\n",
            "train Loss: 3.1379 Acc: 0.2180\n",
            "val Loss: 3.5482 Acc: 0.1872\n",
            "test Loss: 16.9011 Acc: 0.0033\n",
            "\n",
            "Epoch 84/100\n",
            "----------\n",
            "train Loss: 3.1336 Acc: 0.2177\n",
            "val Loss: 2.9447 Acc: 0.2843\n",
            "test Loss: 21.8117 Acc: 0.0000\n",
            "\n",
            "Epoch 85/100\n",
            "----------\n",
            "train Loss: 3.0958 Acc: 0.2264\n",
            "val Loss: 3.6084 Acc: 0.1626\n",
            "test Loss: 19.9207 Acc: 0.0067\n",
            "\n",
            "Epoch 86/100\n",
            "----------\n",
            "train Loss: 3.1103 Acc: 0.2249\n",
            "val Loss: 4.1665 Acc: 0.1226\n",
            "test Loss: 18.5916 Acc: 0.0000\n",
            "\n",
            "Epoch 87/100\n",
            "----------\n",
            "train Loss: 3.0598 Acc: 0.2283\n",
            "val Loss: 4.7211 Acc: 0.0928\n",
            "test Loss: 17.9020 Acc: 0.0000\n",
            "\n",
            "Epoch 88/100\n",
            "----------\n",
            "train Loss: 3.0269 Acc: 0.2346\n",
            "val Loss: 3.7404 Acc: 0.1523\n",
            "test Loss: 19.3580 Acc: 0.0067\n",
            "\n",
            "Epoch 89/100\n",
            "----------\n",
            "train Loss: 3.0164 Acc: 0.2376\n",
            "val Loss: 2.9311 Acc: 0.3013\n",
            "test Loss: 18.4127 Acc: 0.0067\n",
            "\n",
            "Epoch 90/100\n",
            "----------\n",
            "train Loss: 2.9761 Acc: 0.2471\n",
            "val Loss: 3.8454 Acc: 0.1387\n",
            "test Loss: 17.9627 Acc: 0.0067\n",
            "\n",
            "Epoch 91/100\n",
            "----------\n",
            "train Loss: 2.9477 Acc: 0.2517\n",
            "val Loss: 3.5411 Acc: 0.1898\n",
            "test Loss: 18.2141 Acc: 0.0000\n",
            "\n",
            "Epoch 92/100\n",
            "----------\n",
            "train Loss: 2.9139 Acc: 0.2534\n",
            "val Loss: 3.0748 Acc: 0.2596\n",
            "test Loss: 19.3075 Acc: 0.0000\n",
            "\n",
            "Epoch 93/100\n",
            "----------\n",
            "train Loss: 2.8960 Acc: 0.2643\n",
            "val Loss: 3.7554 Acc: 0.1728\n",
            "test Loss: 20.2106 Acc: 0.0000\n",
            "\n",
            "Epoch 94/100\n",
            "----------\n",
            "train Loss: 2.8216 Acc: 0.2843\n",
            "val Loss: 3.7719 Acc: 0.1711\n",
            "test Loss: 20.8571 Acc: 0.0000\n",
            "\n",
            "Epoch 95/100\n",
            "----------\n",
            "train Loss: 2.8415 Acc: 0.2784\n",
            "val Loss: 2.9513 Acc: 0.2647\n",
            "test Loss: 18.1926 Acc: 0.0000\n",
            "\n",
            "Epoch 96/100\n",
            "----------\n",
            "train Loss: 2.8184 Acc: 0.2810\n",
            "val Loss: 4.5418 Acc: 0.1183\n",
            "test Loss: 17.7781 Acc: 0.0067\n",
            "\n",
            "Epoch 97/100\n",
            "----------\n",
            "train Loss: 2.7712 Acc: 0.2837\n",
            "val Loss: 3.1276 Acc: 0.2400\n",
            "test Loss: 19.1785 Acc: 0.0000\n",
            "\n",
            "Epoch 98/100\n",
            "----------\n",
            "train Loss: 2.7371 Acc: 0.2976\n",
            "val Loss: 2.7369 Acc: 0.2987\n",
            "test Loss: 20.0077 Acc: 0.0000\n",
            "\n",
            "Epoch 99/100\n",
            "----------\n",
            "train Loss: 2.7640 Acc: 0.2899\n",
            "val Loss: 3.3973 Acc: 0.1974\n",
            "test Loss: 22.2217 Acc: 0.0033\n",
            "\n",
            "Epoch 100/100\n",
            "----------\n",
            "train Loss: 2.6943 Acc: 0.3063\n",
            "val Loss: 4.1062 Acc: 0.1711\n",
            "test Loss: 19.2680 Acc: 0.0000\n",
            "\n",
            "Training complete in 126m 24s\n",
            "Best val Acc: 0.301277\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhcWioCGFDRc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9713d189-d989-4833-b899-d6344d46a60f"
      },
      "source": [
        "\n",
        "with open(MODEL_NAME+\".csv\", 'w') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow([\"Epoch\", \"Train Accuracy\", \"Validation_Accuracy\", \"Test Accuracy\"])\n",
        "    for i, (val_acc, train_acc, test_acc) in enumerate(zip(hist, train_hist, test_hist)):\n",
        "      writer.writerow([i, train_acc, val_acc, test_acc])\n",
        "\n",
        "with open(MODEL_NAME+\".csv\") as f:\n",
        "  print(f.read())\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch,Train Accuracy,Validation_Accuracy,Test Accuracy\n",
            "0,0.002808510638297872,0.003404255319148936,0.0033333333333333335\n",
            "1,0.004340425531914893,0.00851063829787234,0.0033333333333333335\n",
            "2,0.005702127659574468,0.010212765957446808,0.02\n",
            "3,0.006468085106382979,0.00851063829787234,0.006666666666666667\n",
            "4,0.00774468085106383,0.011063829787234043,0.006666666666666667\n",
            "5,0.009531914893617021,0.009361702127659575,0.0\n",
            "6,0.009957446808510639,0.011063829787234043,0.006666666666666667\n",
            "7,0.01148936170212766,0.013617021276595745,0.0\n",
            "8,0.013531914893617021,0.011063829787234043,0.0\n",
            "9,0.015659574468085108,0.016170212765957447,0.0\n",
            "10,0.016595744680851062,0.017872340425531916,0.006666666666666667\n",
            "11,0.01795744680851064,0.01446808510638298,0.006666666666666667\n",
            "12,0.017361702127659574,0.017872340425531916,0.006666666666666667\n",
            "13,0.020085106382978724,0.02553191489361702,0.02\n",
            "14,0.02051063829787234,0.028085106382978724,0.0\n",
            "15,0.02229787234042553,0.01957446808510638,0.0\n",
            "16,0.022468085106382978,0.033191489361702124,0.0\n",
            "17,0.025872340425531916,0.02723404255319149,0.0\n",
            "18,0.02970212765957447,0.029787234042553193,0.0\n",
            "19,0.028851063829787235,0.020425531914893616,0.0\n",
            "20,0.027659574468085105,0.04085106382978723,0.0\n",
            "21,0.033957446808510636,0.03148936170212766,0.0\n",
            "22,0.03523404255319149,0.028085106382978724,0.0\n",
            "23,0.03404255319148936,0.030638297872340424,0.0\n",
            "24,0.03523404255319149,0.03489361702127659,0.0\n",
            "25,0.03948936170212766,0.04851063829787234,0.0\n",
            "26,0.04340425531914894,0.030638297872340424,0.0\n",
            "27,0.04553191489361702,0.05276595744680851,0.0\n",
            "28,0.04697872340425532,0.04595744680851064,0.0\n",
            "29,0.05242553191489362,0.04680851063829787,0.0\n",
            "30,0.0505531914893617,0.03829787234042553,0.0\n",
            "31,0.05676595744680851,0.04851063829787234,0.0\n",
            "32,0.05897872340425532,0.04170212765957447,0.0\n",
            "33,0.06051063829787234,0.04680851063829787,0.0\n",
            "34,0.0585531914893617,0.06042553191489362,0.0\n",
            "35,0.06170212765957447,0.06382978723404255,0.0\n",
            "36,0.06680851063829787,0.059574468085106386,0.0\n",
            "37,0.06646808510638298,0.07914893617021276,0.0\n",
            "38,0.06953191489361703,0.06893617021276596,0.0\n",
            "39,0.0691063829787234,0.04936170212765958,0.0\n",
            "40,0.07446808510638298,0.03148936170212766,0.0\n",
            "41,0.07642553191489361,0.06468085106382979,0.0\n",
            "42,0.07668085106382978,0.06127659574468085,0.0\n",
            "43,0.07940425531914894,0.07319148936170213,0.0\n",
            "44,0.08272340425531916,0.03914893617021276,0.0\n",
            "45,0.08953191489361702,0.1046808510638298,0.0\n",
            "46,0.0865531914893617,0.062127659574468086,0.0\n",
            "47,0.09489361702127659,0.07148936170212766,0.0\n",
            "48,0.09480851063829787,0.10127659574468086,0.0\n",
            "49,0.09463829787234043,0.13446808510638297,0.0\n",
            "50,0.09463829787234043,0.09446808510638298,0.0\n",
            "51,0.09846808510638298,0.059574468085106386,0.0\n",
            "52,0.10161702127659575,0.09361702127659574,0.0\n",
            "53,0.10748936170212765,0.04,0.0\n",
            "54,0.1094468085106383,0.08595744680851064,0.0\n",
            "55,0.11004255319148937,0.05446808510638298,0.0\n",
            "56,0.11812765957446808,0.09446808510638298,0.0\n",
            "57,0.11863829787234043,0.1251063829787234,0.0\n",
            "58,0.12357446808510639,0.15914893617021278,0.0\n",
            "59,0.12876595744680852,0.10638297872340426,0.0\n",
            "60,0.12519148936170213,0.06297872340425532,0.006666666666666667\n",
            "61,0.12885106382978723,0.1574468085106383,0.0\n",
            "62,0.13540425531914893,0.03404255319148936,0.0\n",
            "63,0.13489361702127659,0.09702127659574468,0.0\n",
            "64,0.14272340425531915,0.1123404255319149,0.0\n",
            "65,0.1417872340425532,0.06127659574468085,0.0\n",
            "66,0.1549787234042553,0.04425531914893617,0.006666666666666667\n",
            "67,0.14663829787234042,0.09191489361702128,0.0\n",
            "68,0.1625531914893617,0.10212765957446808,0.0\n",
            "69,0.15846808510638297,0.11914893617021277,0.0\n",
            "70,0.15651063829787235,0.05787234042553192,0.0\n",
            "71,0.17004255319148937,0.08936170212765958,0.0\n",
            "72,0.16519148936170214,0.0748936170212766,0.0\n",
            "73,0.17191489361702128,0.03148936170212766,0.0\n",
            "74,0.17582978723404255,0.12680851063829787,0.0\n",
            "75,0.17497872340425533,0.20085106382978724,0.0\n",
            "76,0.1883404255319149,0.19063829787234043,0.0\n",
            "77,0.1908936170212766,0.0851063829787234,0.0\n",
            "78,0.19327659574468084,0.05702127659574468,0.006666666666666667\n",
            "79,0.2028936170212766,0.18723404255319148,0.006666666666666667\n",
            "80,0.20629787234042554,0.27063829787234045,0.0\n",
            "81,0.212,0.20680851063829786,0.0\n",
            "82,0.21804255319148935,0.18723404255319148,0.0033333333333333335\n",
            "83,0.21770212765957447,0.28425531914893615,0.0\n",
            "84,0.22638297872340427,0.1625531914893617,0.006666666666666667\n",
            "85,0.22485106382978723,0.1225531914893617,0.0\n",
            "86,0.2283404255319149,0.09276595744680852,0.0\n",
            "87,0.2345531914893617,0.1523404255319149,0.006666666666666667\n",
            "88,0.23761702127659576,0.30127659574468085,0.006666666666666667\n",
            "89,0.24714893617021277,0.13872340425531915,0.006666666666666667\n",
            "90,0.25174468085106383,0.18978723404255318,0.0\n",
            "91,0.25336170212765957,0.25957446808510637,0.0\n",
            "92,0.2643404255319149,0.1727659574468085,0.0\n",
            "93,0.2843404255319149,0.17106382978723406,0.0\n",
            "94,0.27838297872340423,0.26468085106382977,0.0\n",
            "95,0.28102127659574466,0.11829787234042553,0.006666666666666667\n",
            "96,0.28374468085106386,0.24,0.0\n",
            "97,0.29761702127659573,0.2987234042553191,0.0\n",
            "98,0.2898723404255319,0.1974468085106383,0.0033333333333333335\n",
            "99,0.30629787234042555,0.17106382978723406,0.0\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}